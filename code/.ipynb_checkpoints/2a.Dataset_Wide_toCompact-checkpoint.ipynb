{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import distutils\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Input Size:  (1101, 74)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = pd.read_csv(\"../data/processed/gw_stage2.CSV\",\n",
    "                    parse_dates=['Datetime'],\n",
    "                    index_col=['Datetime'])\n",
    "print('Initial Input Size: ',X.shape)\n",
    "y = pd.read_csv(\"../data/processed/spring.CSV\",\n",
    "                    parse_dates=['datetime'],\n",
    "                    index_col=['datetime'])\n",
    "\n",
    "X.head(10)\n",
    "\n",
    "key_value = np.loadtxt(\"../data/processed/NameKey2.CSV\", dtype= \"str\", delimiter=\",\", skiprows=1)\n",
    "stationNames = { k:v for k,v in key_value }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Dict Keys as an int and a string\n",
    "keyint=list(range(0,74))\n",
    "#print(keyint)\n",
    "key_str=[str(x) for x in keyint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'USGS-LESTER LEWIS/S788',\n",
       " '1': 'NWFWMD-BIKE TRAIL / NITRATE POT. MAP/S795',\n",
       " '2': 'NWFWMD-BRADFORD BROOK DEEP/S793',\n",
       " '3': 'NWFWMD-BRADFORD BROOK SHALLOW',\n",
       " '4': 'TOM BROWN TEST/S791',\n",
       " '5': 'USGS-OLSON RD./S677',\n",
       " '6': 'NWFWMD-HQ FLORIDAN OBS/S704',\n",
       " '7': 'USGS-LAKE JACKSON INTERMEDIATE/S706',\n",
       " '8': 'NWFWMD-NITRATE 1',\n",
       " '9': 'NWFWMD-NITRATE 2/S790',\n",
       " '10': 'NWFWMD-NITRATE 4/S782',\n",
       " '11': 'NWFWMD-NITRATE 5/S796',\n",
       " '12': 'NWFWMD-NITRATE 6/S797',\n",
       " '13': 'WAKULLA CORRECTIONAL INSTITUTE 1/S779',\n",
       " '14': 'WAKULLA CONDUIT WELL@WEST OF HWY 61',\n",
       " '15': 'FGS - SULLIVAN SINK 1',\n",
       " '16': 'NWFWMD GW-2F-LZ (FT BRADEN)/S802',\n",
       " '17': 'NWFWMD GW-3F-LZ (ANF FR-305)',\n",
       " '18': 'NWFWMD GW-4F-UZ (HWY27 DOT)',\n",
       " '19': 'NWFWMD GW-5F (LEON CTY VFD)/S814',\n",
       " '20': 'NWFWMD GW-6F (MICC GREENWAY)/S816',\n",
       " '21': 'NWFWMD GW-7F (LEON CR-59)/S818',\n",
       " '22': 'NWFWMD GW-5S (LEON CTY VFD)',\n",
       " '23': 'NWFWMD GW-7S (LEON CR-59)',\n",
       " '24': 'NWFWMD GW-4F-LZ (HWY27 DOT)/S812',\n",
       " '25': 'NWFWMD GW-12F (RIVERSPRINGS MS)/S820',\n",
       " '26': 'NWFWMD GW-2S (FT BRADEN)/S805',\n",
       " '27': 'NWFWMD GW-6S (MICC GREENWAY)',\n",
       " '28': 'NWFWMD GW-2F-UZ (FT BRADEN)/S826',\n",
       " '29': 'NWFWMD GW-3F-UZ (ANF FR-305)',\n",
       " '30': 'AMES SINK/S555',\n",
       " '31': 'MUNSON SLOUGH @ CAPITAL CIRCLE',\n",
       " '32': 'FORDS ARM TRIB@MERD',\n",
       " '33': 'ALFORD ARM TRIB@BUCK',\n",
       " '34': 'CENTRAL D.D.@ORANGE',\n",
       " '35': 'WEST DRAINAGE DITCH @ ROBERTS AVE',\n",
       " '36': 'LAKE KANTURK OUTFALL',\n",
       " '37': 'LAKE LAFAYETTE OUTFALL',\n",
       " '38': 'NE D.D.@MAHAN DR',\n",
       " '39': 'PARK AV D.D.@ VIOLET - S100',\n",
       " '40': 'LAKE JACKSON FACILITY AKA 64/606',\n",
       " '41': 'NE Drainage Ditch @ Centerville and Blairstone',\n",
       " '42': 'Central Drainage Ditch @ Lake Bradford Road',\n",
       " '43': 'Boone Blvd Holding pond (City ID 700)',\n",
       " '44': 'St. Marks River @ San Marcos de Apalachee S.P.',\n",
       " '45': 'Still Creek at Capitola Rd',\n",
       " '46': 'Lake Iamonia Outfall @ Meridian Rd',\n",
       " '47': 'LEON HIGH SCHOOL STORMWATER INLET',\n",
       " '48': 'LAKE BRADFORD',\n",
       " '49': 'ST. MARKS RIVER @ TRAM RD',\n",
       " '50': 'LAKE KINSALE OUTFALL',\n",
       " '51': 'LAKE MUNSON OUTFALL',\n",
       " '52': 'MUNSON SLOUGH @ OAKRIDGE ROAD',\n",
       " '53': 'BLACK CREEK @ CR 267',\n",
       " '54': 'LOST CREEK @ ARRAN RD (CR 368)',\n",
       " '55': 'BRADFORD BROOK @ AENON CHURCH ROAD',\n",
       " '56': 'WEST DRAINAGE DITCH @ US 90',\n",
       " '57': 'ALFORD ARM TRIB@MICC',\n",
       " '58': 'EAST D.D.@ADAMS ST',\n",
       " '59': 'QUINCY CREEK@SR 267',\n",
       " '60': 'FISHER CREEK @ SPRINGHILL RD.',\n",
       " '61': 'WAKULLA RIVER AT BOAT TRAM',\n",
       " '62': 'John Knox Pond Megginnis Tributary',\n",
       " '63': 'Lake Jackson - Miller Landing Road',\n",
       " '64': 'Lauder Pond near Thomasville/Bannerman Rd',\n",
       " '65': 'Regional Stormwater Facility - City of Tallahassee',\n",
       " '66': 'Southwood SW Pond SB111B',\n",
       " '67': 'Southwood SW Pond WD005',\n",
       " '68': 'Lake Miccosukee @ US90 Outfall',\n",
       " '69': 'LAKE IAMONIA EAST',\n",
       " '70': 'LAKE KILLARNY AT MCLAUGHLIN DRIVE',\n",
       " '71': 'LAKE KANTURK AT CLIFDEN DR',\n",
       " '72': 'INDIAN SPRING RUN AT SR61',\n",
       " '73': 'ST. MARKS RIVER - UPPER DISCHARGE BEFORE SWALLET'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stationNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station_Name</th>\n",
       "      <th>key</th>\n",
       "      <th>ts_type</th>\n",
       "      <th>start_timestamp</th>\n",
       "      <th>frequency</th>\n",
       "      <th>h_ts</th>\n",
       "      <th>series_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Station_Name, key, ts_type, start_timestamp, frequency, h_ts, series_length]\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set up new Df\n",
    "new_df=pd.DataFrame(columns=['Station_Name', \"key\", 'ts_type',\"start_timestamp\", 'frequency', 'h_ts', 'series_length'])\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_dict=[]\n",
    "for i in range(74):\n",
    "    if i<=29:\n",
    "        station_type='GW'\n",
    "    else:\n",
    "        station_type='SW'\n",
    "    a=stationNames[key_str[i]]\n",
    "    #print(a)\n",
    "    test=X.loc[:,a]\n",
    "    z=test.tolist()\n",
    "    lst_dict.append({'Station_Name': test.name, 'key':key_str[i], 'ts_type': station_type, \n",
    "                       'start_timestamp':\"2017-01-27\", 'frequency':\"1D\", 'h_ts':z, \n",
    "                       'series_length': test.size})\n",
    "    df2=new_df.append(lst_dict)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station_Name</th>\n",
       "      <th>key</th>\n",
       "      <th>ts_type</th>\n",
       "      <th>start_timestamp</th>\n",
       "      <th>frequency</th>\n",
       "      <th>h_ts</th>\n",
       "      <th>series_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USGS-LESTER LEWIS/S788</td>\n",
       "      <td>0</td>\n",
       "      <td>GW</td>\n",
       "      <td>2017-01-27</td>\n",
       "      <td>1D</td>\n",
       "      <td>[6.340833333, 6.220729167, 6.1534375, 6.046041...</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NWFWMD-BIKE TRAIL / NITRATE POT. MAP/S795</td>\n",
       "      <td>1</td>\n",
       "      <td>GW</td>\n",
       "      <td>2017-01-27</td>\n",
       "      <td>1D</td>\n",
       "      <td>[15.02541667, 15.08229167, 15.1109375, 15.0711...</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NWFWMD-BRADFORD BROOK DEEP/S793</td>\n",
       "      <td>2</td>\n",
       "      <td>GW</td>\n",
       "      <td>2017-01-27</td>\n",
       "      <td>1D</td>\n",
       "      <td>[20.72239583, 20.79333333, 20.8790625, 20.8895...</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NWFWMD-BRADFORD BROOK SHALLOW</td>\n",
       "      <td>3</td>\n",
       "      <td>GW</td>\n",
       "      <td>2017-01-27</td>\n",
       "      <td>1D</td>\n",
       "      <td>[38.61302083, 38.61760417, 38.6271875, 38.5586...</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TOM BROWN TEST/S791</td>\n",
       "      <td>4</td>\n",
       "      <td>GW</td>\n",
       "      <td>2017-01-27</td>\n",
       "      <td>1D</td>\n",
       "      <td>[25.915625, 26.0184375, 26.03791667, 25.949375...</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>LAKE KILLARNY AT MCLAUGHLIN DRIVE</td>\n",
       "      <td>70</td>\n",
       "      <td>SW</td>\n",
       "      <td>2017-01-27</td>\n",
       "      <td>1D</td>\n",
       "      <td>[73.27979167, 73.21723959, 73.16114583, 73.118...</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>LAKE KANTURK AT CLIFDEN DR</td>\n",
       "      <td>71</td>\n",
       "      <td>SW</td>\n",
       "      <td>2017-01-27</td>\n",
       "      <td>1D</td>\n",
       "      <td>[73.31546875, 73.25291667, 73.19697917, 73.152...</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>INDIAN SPRING RUN AT SR61</td>\n",
       "      <td>72</td>\n",
       "      <td>SW</td>\n",
       "      <td>2017-01-27</td>\n",
       "      <td>1D</td>\n",
       "      <td>[5.243125, 5.170104167, 5.105208333, 5.0539583...</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>ST. MARKS RIVER - UPPER DISCHARGE BEFORE SWALLET</td>\n",
       "      <td>73</td>\n",
       "      <td>SW</td>\n",
       "      <td>2017-01-27</td>\n",
       "      <td>1D</td>\n",
       "      <td>[9.488892223, 9.488892223, 9.488892223, 9.4888...</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Wakulla_Springs_USGS</td>\n",
       "      <td>74</td>\n",
       "      <td>spring</td>\n",
       "      <td>2017-01-27</td>\n",
       "      <td>1D</td>\n",
       "      <td>[5.470909091, 5.418020833, 5.3453125, 5.237916...</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Station_Name key ts_type  \\\n",
       "0                             USGS-LESTER LEWIS/S788   0      GW   \n",
       "1          NWFWMD-BIKE TRAIL / NITRATE POT. MAP/S795   1      GW   \n",
       "2                    NWFWMD-BRADFORD BROOK DEEP/S793   2      GW   \n",
       "3                      NWFWMD-BRADFORD BROOK SHALLOW   3      GW   \n",
       "4                                TOM BROWN TEST/S791   4      GW   \n",
       "..                                               ...  ..     ...   \n",
       "70                 LAKE KILLARNY AT MCLAUGHLIN DRIVE  70      SW   \n",
       "71                        LAKE KANTURK AT CLIFDEN DR  71      SW   \n",
       "72                         INDIAN SPRING RUN AT SR61  72      SW   \n",
       "73  ST. MARKS RIVER - UPPER DISCHARGE BEFORE SWALLET  73      SW   \n",
       "74                              Wakulla_Springs_USGS  74  spring   \n",
       "\n",
       "   start_timestamp frequency  \\\n",
       "0       2017-01-27        1D   \n",
       "1       2017-01-27        1D   \n",
       "2       2017-01-27        1D   \n",
       "3       2017-01-27        1D   \n",
       "4       2017-01-27        1D   \n",
       "..             ...       ...   \n",
       "70      2017-01-27        1D   \n",
       "71      2017-01-27        1D   \n",
       "72      2017-01-27        1D   \n",
       "73      2017-01-27        1D   \n",
       "74      2017-01-27        1D   \n",
       "\n",
       "                                                 h_ts series_length  \n",
       "0   [6.340833333, 6.220729167, 6.1534375, 6.046041...          1101  \n",
       "1   [15.02541667, 15.08229167, 15.1109375, 15.0711...          1101  \n",
       "2   [20.72239583, 20.79333333, 20.8790625, 20.8895...          1101  \n",
       "3   [38.61302083, 38.61760417, 38.6271875, 38.5586...          1101  \n",
       "4   [25.915625, 26.0184375, 26.03791667, 25.949375...          1101  \n",
       "..                                                ...           ...  \n",
       "70  [73.27979167, 73.21723959, 73.16114583, 73.118...          1101  \n",
       "71  [73.31546875, 73.25291667, 73.19697917, 73.152...          1101  \n",
       "72  [5.243125, 5.170104167, 5.105208333, 5.0539583...          1101  \n",
       "73  [9.488892223, 9.488892223, 9.488892223, 9.4888...          1101  \n",
       "74  [5.470909091, 5.418020833, 5.3453125, 5.237916...          1101  \n",
       "\n",
       "[75 rows x 7 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=y['Gage_Height'].tolist()\n",
    "spring_dict=[{'Station_Name': \"Wakulla_Springs_USGS\", 'key':74, 'ts_type': \"spring\", \n",
    "                       'start_timestamp':\"2017-01-27\", 'frequency':\"1D\", 'h_ts': z, \n",
    "                       'series_length': len(z)}]\n",
    "df2=df2.append(spring_dict, ignore_index=True)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowTypeError",
     "evalue": "(\"Expected bytes, got a 'int' object\", 'Conversion failed for column key with type object')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-cc6b01086925>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'../data/processed/GW_SW_daily'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'pyarrow'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mto_parquet\u001b[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   2683\u001b[0m             \u001b[0mpartition_cols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartition_cols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2684\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2685\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2686\u001b[0m         )\n\u001b[0;32m   2687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[0mpartition_cols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartition_cols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m         \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m     )\n\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0mfrom_pandas_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"preserve_index\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfrom_pandas_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         path_or_handle, handles, kwargs[\"filesystem\"] = _get_path_or_handle(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyarrow\\table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyarrow\\pandas_compat.py\u001b[0m in \u001b[0;36mdataframe_to_arrays\u001b[1;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[0;32m    610\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnthreads\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m         arrays = [convert_column(c, f)\n\u001b[1;32m--> 612\u001b[1;33m                   for c, f in zip(columns_to_convert, convert_fields)]\n\u001b[0m\u001b[0;32m    613\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyarrow\\pandas_compat.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    610\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnthreads\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m         arrays = [convert_column(c, f)\n\u001b[1;32m--> 612\u001b[1;33m                   for c, f in zip(columns_to_convert, convert_fields)]\n\u001b[0m\u001b[0;32m    613\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyarrow\\pandas_compat.py\u001b[0m in \u001b[0;36mconvert_column\u001b[1;34m(col, field)\u001b[0m\n\u001b[0;32m    596\u001b[0m             e.args += (\"Conversion failed for column {!s} with type {!s}\"\n\u001b[0;32m    597\u001b[0m                        .format(col.name, col.dtype),)\n\u001b[1;32m--> 598\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfield_nullable\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnull_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m             raise ValueError(\"Field {} was non-nullable but pandas column \"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyarrow\\pandas_compat.py\u001b[0m in \u001b[0;36mconvert_column\u001b[1;34m(col, field)\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_pandas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m         except (pa.ArrowInvalid,\n\u001b[0;32m    594\u001b[0m                 \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mArrowNotImplementedError\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyarrow\\array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyarrow\\array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyarrow\\error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowTypeError\u001b[0m: (\"Expected bytes, got a 'int' object\", 'Conversion failed for column key with type object')"
     ]
    }
   ],
   "source": [
    "df2.to_parquet(path='../data/processed/GW_SW_daily', engine = 'pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_compact_to_ts(\n",
    "    df: pd.DataFrame,\n",
    "    filename: str,\n",
    "    static_columns: list,\n",
    "    time_varying_columns: list,\n",
    "    sep: str = \";\",\n",
    "    encoding: str = \"utf-8\",\n",
    "    date_format: str = \"%Y-%m-%d %H-%M-%S\",\n",
    "    chunk_size: int = 50,\n",
    "):\n",
    "    \"\"\"Writes a dataframe in the compact form to disk\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe in compact form\n",
    "        filename (str): Filename to which the dataframe should be written to\n",
    "        static_columns (list): List of column names of static features\n",
    "        time_varying_columns (list): List of column names of time varying columns\n",
    "        sep (str, optional): Separator with which the arrays are stored in the text file. Defaults to \";\".\n",
    "        encoding (str, optional): encoding of the text file. Defaults to \"utf-8\".\n",
    "        date_format (str, optional): Format in which datetime shud be written out in text file. Defaults to \"%Y-%m-%d %H-%M-%S\".\n",
    "        chunk_size (int, optional): Chunk size while writing files to disk. Defaults to 50.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if sep == \":\":\n",
    "        warnings.warn(\n",
    "            \"Using `:` as separator will not work well if `:` is present in the string representation of date time.\"\n",
    "        )\n",
    "    with open(filename, \"w\", encoding=encoding) as f:\n",
    "        for c, dtype in df.dtypes.items():\n",
    "            if c in static_columns:\n",
    "                typ = \"static\"\n",
    "            elif c in time_varying_columns:\n",
    "                typ = \"time_varying\"\n",
    "            f.write(f\"@column {c} {dtype.name} {typ}\")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(f\"@data\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        def write_ts(x):\n",
    "            l = \"\"\n",
    "            for c in x.index:\n",
    "                if isinstance(x[c], np.ndarray):\n",
    "                    l += \"|\".join(x[c].astype(str))\n",
    "                    l += sep\n",
    "                elif isinstance(x[c], pd.Timestamp):\n",
    "                    l += x[c].strftime(date_format)\n",
    "                    l += sep\n",
    "                else:\n",
    "                    l += str(x[c])\n",
    "                    l += sep\n",
    "            l += \"\\n\"\n",
    "            return l\n",
    "\n",
    "        [\n",
    "            f.writelines([write_ts(x.loc[i]) for i in tqdm(x.index)])\n",
    "            for x in tqdm(\n",
    "                np.split(df, np.arange(chunk_size, len(df), chunk_size)),\n",
    "                desc=\"Writing in Chunks...\",\n",
    "            )\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing in Chunks...:   0%|                                                                      | 0/2 [00:00<?, ?it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 847.72it/s]\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 926.22it/s]\u001b[A\n",
      "Writing in Chunks...: 100%|██████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 20.41it/s]\n"
     ]
    }
   ],
   "source": [
    "write_compact_to_ts(df2,\n",
    "    static_columns = ['Station_Name', 'key', 'ts_type','series_length', 'frequency', 'start_timestamp'], \n",
    "    time_varying_columns = ['h_ts'],\n",
    "    filename=f\"../data/processed/GW_SW_Daily.ts\",\n",
    "    sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compact_to_expanded(\n",
    "    df, timeseries_col, static_cols, time_varying_cols, ts_identifier):\n",
    "    def preprocess_expanded(x):\n",
    "        ### Fill missing dates with NaN ###\n",
    "        # Create a date range from  start\n",
    "        dr = pd.date_range(\n",
    "            start=x[\"start_timestamp\"],\n",
    "            periods=len(x[\"h_ts\"]),\n",
    "            freq=x[\"frequency\"],\n",
    "        )\n",
    "        df_columns = defaultdict(list)\n",
    "        df_columns[\"timestamp\"] = dr\n",
    "        for col in [ts_identifier, timeseries_col] + static_cols + time_varying_cols:\n",
    "            df_columns[col] = x[col]\n",
    "        return pd.DataFrame(df_columns)\n",
    "        \n",
    "\n",
    "    all_series = []\n",
    "    for i in tqdm(range(len(df))):\n",
    "        all_series.append(preprocess_expanded(df.iloc[i]))\n",
    "    df = pd.concat(all_series)\n",
    "    del all_series\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 75/75 [00:00<00:00, 765.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Station_Name</th>\n",
       "      <th>h_ts</th>\n",
       "      <th>key</th>\n",
       "      <th>ts_type</th>\n",
       "      <th>series_length</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-27</td>\n",
       "      <td>USGS-LESTER LEWIS/S788</td>\n",
       "      <td>6.340833</td>\n",
       "      <td>0</td>\n",
       "      <td>GW</td>\n",
       "      <td>1101</td>\n",
       "      <td>1D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-28</td>\n",
       "      <td>USGS-LESTER LEWIS/S788</td>\n",
       "      <td>6.220729</td>\n",
       "      <td>0</td>\n",
       "      <td>GW</td>\n",
       "      <td>1101</td>\n",
       "      <td>1D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-29</td>\n",
       "      <td>USGS-LESTER LEWIS/S788</td>\n",
       "      <td>6.153437</td>\n",
       "      <td>0</td>\n",
       "      <td>GW</td>\n",
       "      <td>1101</td>\n",
       "      <td>1D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-30</td>\n",
       "      <td>USGS-LESTER LEWIS/S788</td>\n",
       "      <td>6.046042</td>\n",
       "      <td>0</td>\n",
       "      <td>GW</td>\n",
       "      <td>1101</td>\n",
       "      <td>1D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>USGS-LESTER LEWIS/S788</td>\n",
       "      <td>5.991562</td>\n",
       "      <td>0</td>\n",
       "      <td>GW</td>\n",
       "      <td>1101</td>\n",
       "      <td>1D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>2020-01-28</td>\n",
       "      <td>Wakulla_Springs_USGS</td>\n",
       "      <td>4.814896</td>\n",
       "      <td>74</td>\n",
       "      <td>spring</td>\n",
       "      <td>1101</td>\n",
       "      <td>1D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>2020-01-29</td>\n",
       "      <td>Wakulla_Springs_USGS</td>\n",
       "      <td>4.811146</td>\n",
       "      <td>74</td>\n",
       "      <td>spring</td>\n",
       "      <td>1101</td>\n",
       "      <td>1D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>2020-01-30</td>\n",
       "      <td>Wakulla_Springs_USGS</td>\n",
       "      <td>4.807292</td>\n",
       "      <td>74</td>\n",
       "      <td>spring</td>\n",
       "      <td>1101</td>\n",
       "      <td>1D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>Wakulla_Springs_USGS</td>\n",
       "      <td>4.777188</td>\n",
       "      <td>74</td>\n",
       "      <td>spring</td>\n",
       "      <td>1101</td>\n",
       "      <td>1D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>Wakulla_Springs_USGS</td>\n",
       "      <td>4.794500</td>\n",
       "      <td>74</td>\n",
       "      <td>spring</td>\n",
       "      <td>1101</td>\n",
       "      <td>1D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82575 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      timestamp            Station_Name      h_ts key ts_type  series_length  \\\n",
       "0    2017-01-27  USGS-LESTER LEWIS/S788  6.340833   0      GW           1101   \n",
       "1    2017-01-28  USGS-LESTER LEWIS/S788  6.220729   0      GW           1101   \n",
       "2    2017-01-29  USGS-LESTER LEWIS/S788  6.153437   0      GW           1101   \n",
       "3    2017-01-30  USGS-LESTER LEWIS/S788  6.046042   0      GW           1101   \n",
       "4    2017-01-31  USGS-LESTER LEWIS/S788  5.991562   0      GW           1101   \n",
       "...         ...                     ...       ...  ..     ...            ...   \n",
       "1096 2020-01-28    Wakulla_Springs_USGS  4.814896  74  spring           1101   \n",
       "1097 2020-01-29    Wakulla_Springs_USGS  4.811146  74  spring           1101   \n",
       "1098 2020-01-30    Wakulla_Springs_USGS  4.807292  74  spring           1101   \n",
       "1099 2020-01-31    Wakulla_Springs_USGS  4.777188  74  spring           1101   \n",
       "1100 2020-02-01    Wakulla_Springs_USGS  4.794500  74  spring           1101   \n",
       "\n",
       "     frequency  \n",
       "0           1D  \n",
       "1           1D  \n",
       "2           1D  \n",
       "3           1D  \n",
       "4           1D  \n",
       "...        ...  \n",
       "1096        1D  \n",
       "1097        1D  \n",
       "1098        1D  \n",
       "1099        1D  \n",
       "1100        1D  \n",
       "\n",
       "[82575 rows x 7 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_df=compact_to_expanded(df2, timeseries_col='h_ts', \n",
    "                          static_cols=['key', 'ts_type', 'series_length', 'frequency'],\n",
    "                          time_varying_cols= [],\n",
    "                          ts_identifier='Station_Name')\n",
    "exp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory_footprint(df):\n",
    "    dtypes = df.dtypes\n",
    "    object_cols = dtypes[dtypes == \"object\"].index.tolist()\n",
    "    float_cols = dtypes[dtypes == \"float64\"].index.tolist()\n",
    "    int_cols = dtypes[dtypes == \"int64\"].index.tolist()\n",
    "    df[int_cols] = df[int_cols].astype(\"int32\")\n",
    "    df[object_cols] = df[object_cols].astype(\"category\")\n",
    "    df[float_cols] = df[float_cols].astype(\"float32\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 82575 entries, 0 to 1100\n",
      "Columns: 7 entries, timestamp to frequency\n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), object(4)\n",
      "memory usage: 23.1 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 82575 entries, 0 to 1100\n",
      "Columns: 7 entries, timestamp to frequency\n",
      "dtypes: category(4), datetime64[ns](1), float32(1), int32(1)\n",
      "memory usage: 2.2 MB\n"
     ]
    }
   ],
   "source": [
    "exp_df.info(memory_usage=\"deep\", verbose=False)\n",
    "exp_df = reduce_memory_footprint(exp_df)\n",
    "\n",
    "exp_df.info(memory_usage=\"deep\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Training samples: 49575 | # of Validation samples: 16500 | # of Test samples: 16500\n",
      "Max Date in Train: 2018-11-18 00:00:00 | Min Date in Validation: 2018-11-19 00:00:00 | Min Date in Test: 2019-06-27 00:00:00\n"
     ]
    }
   ],
   "source": [
    "### Train/Test Split of 60% Train 20% Val and 20% Test. Dates for these are calcualted outside of the notebook\n",
    "val_mask = (exp_df['timestamp']>= '2018-11-19') & (exp_df['timestamp'] < '2019-06-27')\n",
    "test_mask =(exp_df['timestamp'] >= '2019-06-27')\n",
    "\n",
    "train = exp_df[~(val_mask|test_mask)]\n",
    "val = exp_df[val_mask]\n",
    "test = exp_df[test_mask]\n",
    "print(f\"# of Training samples: {len(train)} | # of Validation samples: {len(val)} | # of Test samples: {len(test)}\")\n",
    "print(f\"Max Date in Train: {train.timestamp.max()} | Min Date in Validation: {val.timestamp.min()} | Min Date in Test: {test.timestamp.min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"../data/processed/daily_train.CSV\")\n",
    "val.to_csv(\"../data/processed/daily_val.CSV\")\n",
    "test.to_csv(\"../data/processed/daily_val.CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
